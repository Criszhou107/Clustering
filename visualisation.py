

def do_pca(X_df, n_components=20, centered=True):

    import numpy as np
    import pandas as pd
    from sklearn.decomposition import PCA
    
    '''
        Return:
            W: PCA loadings for each feature (how much each feature contributes to each PC)
            Xproj: projected data in PCA space (component scores, reduced dimension representation of data)
            fracs: Fraction of data explained by each principal component in a vector
    '''
    n_features = X_df.shape[1] # Original data dimension (no of features)
    if n_components > n_features:
        n_components = n_features
    if isinstance(X_df, pd.DataFrame):
        X = X_df.values #Â Get the data matrix from the X_df dataframe
    if centered:            
        X = X.astype('float')  # Since X is object
        X = X - X.mean(0)
        X = X/X.std(0)
    pca = PCA(n_components=n_components) # run the PCA algorithm from sklearn on data X
    pca.fit(X)
    fracs = pca.explained_variance_ratio_  # vector with explained variance for each principal component (PC)
    Xproj = pca.fit_transform(X)   # Low-dim projection (aka Scores) - n_sample, n_redim
    W_l = pca.components_            # PC Loadings - n_redim, n_feature
    # construct two DataFrames for later use
    # Loadings in a dataframe
    W_df = pd.DataFrame(W_l, index=['PC' + str(i) for i in np.arange(1, W_l.shape[0]+1)], columns=X_df.columns.values)
    # Low-dimensional projection of data in a dataframe
    Xproj_df = pd.DataFrame(Xproj, index=X_df.index, columns=['PC'+str(i) for i in np.arange(1, Xproj.shape[1]+1)])
    W = W_df.T
    return W, Xproj_df, fracs

def pca_biplot(W, scores, data, topN=10, XPC='PC1', YPC='PC2', feature=None):

    import plotly
    import plotly.graph_objs as go
    import plotly.express as px
    import pandas as pd
    import numpy as np
    
    #combined = pd.concat([scores, data], axis=1, sort=False)
    
    FeaturePCMagnitudes = W.loc[:,XPC]**2+W.loc[:,YPC]**2 # Squared length of vectors in the 2D PCA space
    FeaturePCMagnitudes.sort_values(inplace=True, ascending=False) # Sort from largest to smallest
    #topN = 15 # Only show the 15 largest feature vectors

    if feature == None:
        labels = scores.index
        fig = px.scatter(scores, x=XPC, y=YPC, color=labels) # default is to plot the classes
        fig.update_traces(mode='markers', marker_line_width=1, marker_size=8)
    else:                               # otherwise plot the feature given in the function argument
        fig = go.Figure()
        tmp = np.array([feature + " "]*len(data))
        markertext = list()
        for i in range (1,len(data)):
            markertext.append(tmp[i] + str(data[feature].values[i]))
        fig.add_trace(go.Scatter(
            name="Data point",
            x=scores.loc[:,XPC],
            y=scores.loc[:,YPC],
            text = markertext,
            marker=dict(
                line_width=1,
                size=8,
                color=data.loc[:,feature],
                colorbar=dict(
                    title=feature,
                    x=-0.2,
                    y=0.5
                ),
                colorscale="Viridis"
            ),
            mode="markers"))
        fig.update_layout(xaxis_title=XPC, yaxis_title=YPC)

    for i, g in enumerate(FeaturePCMagnitudes.iteritems()): 
        if(topN is not None and i < topN):
            fig.add_scatter(x=[0,W.loc[g[0],XPC]*2.5],y=[0,W.loc[g[0],YPC]*2.5],mode='lines',name=g[0])
            #continue

    return fig

